{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "09c49b31-c4e3-4cd1-99a7-0b6de81d1fb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: OPENAI_API_KEY=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\n"
     ]
    }
   ],
   "source": [
    "%env OPENAI_API_KEY=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"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f0cb5aac-f0a7-4d2d-b825-6a17ba1a962b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dspy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "2a82a7f0-77d4-4b7c-8a53-e9c271496a65",
   "metadata": {},
   "outputs": [],
   "source": [
    "lm = dspy.LM(\"openai/gemma-3n-e2b-it-text\", api_base=\"http://localhost:1234/v1\", api_key=\"foo\")\n",
    "gpt4 = dspy.LM(\"openai/gpt-4.1\")\n",
    "dspy.configure(lm=lm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fe38d215-72fd-412d-bead-a6aa4676124a",
   "metadata": {},
   "outputs": [],
   "source": [
    "summarize = dspy.ChainOfThought(\"abstract -> gist\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "16ccf3e9-d385-4e8e-98ac-cea28b202678",
   "metadata": {},
   "outputs": [],
   "source": [
    "abstract = \"\"\" show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching coifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "7a25c2f0-ee57-4133-8523-aa8921034b8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = summarize(abstract=abstract).gist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "f816685b-e32f-4713-8d81-3d21e65d79f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'GPT-3, a large language model with 175 billion parameters, demonstrates strong few-shot performance on diverse NLP tasks without fine-tuning. It achieves this by using text prompts and demonstrations. The study shows successes in translation, question answering, reasoning, and news generation, but also identifies limitations and potential societal impacts related to training data and performance on specific datasets.'"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "0f105200-15f8-4674-aec5-0748dbeabcac",
   "metadata": {},
   "outputs": [],
   "source": [
    "judge = dspy.ChainOfThought(dspy.Signature(\"abstract, summary -> faithful: bool, helpful: bool\", instructions=\"evaluate the summary of the abstract and state if the summary is helpful and faithful.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "17224321-6b2a-4e12-95c0-5e7bcc26ad88",
   "metadata": {},
   "outputs": [],
   "source": [
    "judge.set_lm(dspy.LM(\"openai/gpt-4.1\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "f1f0b610-67f7-4639-bacd-e24f22afd631",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Prediction(\n",
       "    reasoning='The summary accurately captures the main points of the abstract: it mentions the scale of GPT-3, its strong few-shot performance without fine-tuning, the use of text prompts, and the range of tasks where it excels (translation, question answering, reasoning, news generation). It also notes the identification of limitations and societal impacts, which are discussed in the abstract. The summary omits some specific details (such as the comparison to prior state-of-the-art, the lack of gradient updates, and examples like unscrambling words or 3-digit arithmetic), but these omissions do not misrepresent the content. The summary is concise, faithful to the abstract, and provides a helpful overview.',\n",
       "    faithful=True,\n",
       "    helpful=True\n",
       ")"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "judge(abstract=abstract, summary=summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeb7477d-4f17-4911-b0a4-18e14b9b3346",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
